import spacy
from spacy.tokens import DocBin
from tqdm import tqdm
import json
import pandas as pd
from spacy import displacy
from pathlib import Path
from multiprocessing import Pool
from sklearn.model_selection import train_test_split

#python -m spacy init config ./content/config.cfg --lang pl --pipeline ner --optimize efficiency

def get_spacy_doc(file, data):
  nlp = spacy.blank('pl')
  db = DocBin()
  for text, annot in tqdm(data):
    doc = nlp.make_doc(text)
    annot = annot['entities']

    ents = []
    entity_indices = []
    for start, end, label in annot:
      skip_entity = False
      for idx in range(start, end):
        if idx in entity_indices:
          skip_entity = True
          break
      if skip_entity:
        continue

      entity_indices = entity_indices + list(range(start, end))
      try:
        span = doc.char_span(start, end, label=label, alignment_mode='strict')
      except:
        continue

      if span is None:
        err_data = str([start, end]) + "    " + str(text) + "\n"
        file.write(err_data)
      else:
        ents.append(span)

    try:
      doc.ents = ents
      db.add(doc)
    except:
      pass

  return db

def create_splits(cv_data):
    train, test = train_test_split(cv_data, test_size=0.2)
    print(len(train), len(test))
    file = open('./content/train_file.txt','w')
    db = get_spacy_doc(file, train)
    db.to_disk('./content/train_data.spacy')
    db = get_spacy_doc(file, test)
    db.to_disk('./content/test_data.spacy')
    file.close()

if __name__ == "__main__":
  cv_data = json.load(open('./content/data_transformed.json','r'))
  create_splits(cv_data)
  #python -m spacy train ./content/config.cfg  --output ./content/output  --paths.train ./content/train_data.spacy  --paths.dev ./content/test_data.spacy